{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Prior Distribution of Relaxation Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this tutorial we will reproduce Figure 2 of the article https://iopscience.iop.org/article/10.1149/1945-7111/ab631a/meta\n",
    "\n",
    "DP-DRT is our next newly developed deep learning based approach to obtain the DRT from the EIS data. The DP-DRT is trained on single data point with random input to calculate the DRT that can well match the EIS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "import math\n",
    "from math import sin, cos, pi\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import compute_DRT\n",
    "\n",
    "# check the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Define parameters of the ZARC circuit\n",
    "\n",
    "The impedance of a ZARC can be written as\n",
    "$$\n",
    "Z^{\\rm exact}(f) = R_\\infty + \\displaystyle \\frac{1}{\\displaystyle \\frac{1}{R_{\\rm ct}}+C \\left(i 2\\pi f\\right)^\\phi}\n",
    "$$\n",
    "\n",
    "where $\\displaystyle C = \\frac{\\tau_0^\\phi}{R_{\\rm ct}}$.\n",
    "\n",
    "The analytical DRT can be computed analytically as\n",
    "\n",
    "$$\n",
    "\\gamma(\\log \\tau) =  \\displaystyle \\frac{\\displaystyle R_{\\rm ct}}{\\displaystyle 2\\pi} \\displaystyle \\frac{\\displaystyle \\sin\\left((1-\\phi)\\pi\\right)}{\\displaystyle \\cosh(\\phi \\log(\\tau/\\tau_0))-\\cos(\\pi(1-\\phi))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the seed for random numbers\n",
    "rng = rnd.seed(214975)\n",
    "rng_np = np.random.seed(213912)\n",
    "torch.manual_seed(213912)\n",
    "\n",
    "# define frequency range, from 1E-4 to 1E4 with 10 ppd\n",
    "N_freqs = 81\n",
    "freq_vec = np.logspace(-4., 4., num=N_freqs, endpoint=True)\n",
    "tau_vec  = 1./freq_vec\n",
    "\n",
    "# define parameters for ZARC model and calculate the impedance and gamma following the above equations\n",
    "R_inf = 10\n",
    "R_ct = 50\n",
    "phi = 0.8\n",
    "tau_0 = 1\n",
    "C = tau_0**phi/R_ct\n",
    "\n",
    "Z = R_inf + 1./(1./R_ct+C*(1j*2.*pi*freq_vec)**phi)\n",
    "gamma_exact = (R_ct)/(2.*pi)*sin((1.-phi)*pi)/(np.cosh(phi*np.log(tau_vec/tau_0))-cos((1.-phi)*pi))\n",
    "\n",
    "# adding noise to the impedance data\n",
    "sigma_n_exp = 0.1\n",
    "Z_exp = Z + (sigma_n_exp**2)*np.random.normal(0,1,N_freqs) + 1j*(sigma_n_exp**2)*np.random.normal(0,1,N_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform impedance variables to tensors\n",
    "Z_exp_re_torch = torch.from_numpy(np.real(Z_exp)).type(torch.FloatTensor).reshape(1,N_freqs)\n",
    "Z_exp_im_torch = torch.from_numpy(np.imag(Z_exp)).type(torch.FloatTensor).reshape(1,N_freqs)\n",
    "# tranform gamma\n",
    "gamma_exact_torch = torch.from_numpy(gamma_exact).type(torch.FloatTensor)\n",
    "\n",
    "# define the matrices that calculate the impedace from DRT, i.e., Z_re = A_re * gamma, Z_im = A_im * gamma\n",
    "A_re = compute_DRT.A_re(freq_vec)\n",
    "A_im = compute_DRT.A_im(freq_vec)\n",
    "\n",
    "# transform these matrices into tensors\n",
    "A_re_torch = torch.from_numpy(A_re.T).type(torch.FloatTensor)\n",
    "A_im_torch = torch.from_numpy(A_im.T).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Define the deep learning network structure and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the arbitrary zeta input\n",
    "N_zeta = 1\n",
    "\n",
    "# define the neural network\n",
    "# N is batch size, D_in is input dimension, H is hidden dimension, D_out is output dimension.\n",
    "N = 1\n",
    "D_in = N_zeta\n",
    "H = max(N_freqs,10*N_zeta)\n",
    "# the output also includes the Rct, so it has N_freq+1 dimension\n",
    "D_out = N_freqs+1\n",
    "\n",
    "# Construct the neural network structure\n",
    "class vanilla_model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vanilla_model, self).__init__()\n",
    "        self.fct_1 = torch.nn.Linear(D_in, H)\n",
    "        self.fct_2 = torch.nn.Linear(H, H) \n",
    "        self.fct_3 = torch.nn.Linear(H, H)\n",
    "        self.fct_4 = torch.nn.Linear(H, D_out) \n",
    "        \n",
    "        # initialize the weight parameters\n",
    "        torch.nn.init.xavier_uniform_(self.fct_1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fct_2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fct_3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fct_4.weight)\n",
    "\n",
    "#         torch.nn.init.zeros_(self.fct_1.weight)\n",
    "#         torch.nn.init.zeros_(self.fct_2.weight)\n",
    "#         torch.nn.init.zeros_(self.fct_3.weight)\n",
    "#         torch.nn.init.zeros_(self.fct_4.weight)\n",
    "    # forward\n",
    "    def forward(self, zeta):\n",
    "        h = F.elu(self.fct_1(zeta))\n",
    "        h = F.elu(self.fct_2(h))\n",
    "        h = F.elu(self.fct_3(h))\n",
    "        gamma_pred = F.softplus(self.fct_4(h), beta = 5)        \n",
    "        \n",
    "        return gamma_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output, Z_exp_re_torch, Z_exp_im_torch, A_re_torch, A_im_torch):\n",
    "    \n",
    "    MSE_re = torch.sum((output[:, -1] + torch.mm(output[:, 0:-1], A_re_torch) - Z_exp_re_torch)**2)\n",
    "    MSE_im = torch.sum((torch.mm(output[:, 0:-1], A_im_torch) - Z_exp_im_torch)**2)\n",
    "    MSE = MSE_re + MSE_im\n",
    "    \n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L_torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-423faa6c3f4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Compute the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_exp_re_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_exp_im_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_re_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_im_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL_torch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;31m# save it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mloss_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'L_torch' is not defined"
     ]
    }
   ],
   "source": [
    "model = vanilla_model()\n",
    "\n",
    "# initialize following variables for store\n",
    "zeta = torch.randn(N, N_zeta)\n",
    "loss_vec = np.array([])\n",
    "distance_vec = np.array([])\n",
    "lambda_vec = np.array([])\n",
    "\n",
    "# optimize the neural network\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which tensors it should update.\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# max iterations\n",
    "max_iters = 100001\n",
    "gamma_NN_store = torch.zeros((max_iters, N_freqs))\n",
    "R_inf_NN_store = torch.zeros((max_iters, 1))\n",
    "\n",
    "for t in range(max_iters):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    gamma = model(zeta)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = loss_fn(gamma, Z_exp_re_torch, Z_exp_im_torch, A_re_torch, A_im_torch)\n",
    "    # save it\n",
    "    loss_vec = np.append(loss_vec, loss.item())\n",
    "    \n",
    "    # store gamma\n",
    "    gamma_NN = gamma[:, 0:-1].detach().reshape(-1) \n",
    "    gamma_NN_store[t, :] = gamma_NN\n",
    "    \n",
    "    # store gamma\n",
    "    R_inf_NN_store[t,:] = gamma[:, -1].detach().reshape(-1)\n",
    "\n",
    "    # Compute the distance\n",
    "    distance = math.sqrt(torch.sum((gamma_NN-gamma_exact_torch)**2).item()) \n",
    "    # save it\n",
    "    distance_vec = np.append(distance_vec, distance)\n",
    "\n",
    "    # and print it\n",
    "    if not t%100:\n",
    "        print('iter=', t, '; loss=', loss.item(), '; distance=', distance)\n",
    "\n",
    "    # Before starting the optimizer we can note that the learning rate\n",
    "    # can be modified on the go, see\n",
    "    # https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "    # It would be nice to implement this option in the future.\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_opt = np.argmin(distance_vec)    \n",
    "index_early_stop = np.flatnonzero(np.abs(np.diff(loss_vec))<1E-8)\n",
    "\n",
    "gamma_DIP_torch_opt = gamma_NN_store[index_opt, :]\n",
    "R_inf_DIP_torch_opt = R_inf_NN_store[index_opt, :]\n",
    "\n",
    "gamma_DIP_opt = gamma_DIP_torch_opt.detach().numpy()\n",
    "R_DIP_opt = R_inf_DIP_torch_opt.detach().numpy()\n",
    "\n",
    "if len(index_early_stop):\n",
    "    gamma_DIP_torch_early_stop = gamma_NN_store[index_early_stop[0], :]\n",
    "    gamma_DIP = gamma_DIP_torch_early_stop.detach().numpy()\n",
    "    R_DIP = R_inf_NN_store[index_early_stop[0], :]\n",
    "    R_DIP = R_DIP.detach().numpy()\n",
    "else:\n",
    "    gamma_DIP = gamma_DIP_opt\n",
    "    R_DIP = R_DIP_opt\n",
    "    \n",
    "Z_DIP = R_DIP + np.matmul(A_re, gamma_DIP) + 1j*np.matmul(A_im, gamma_DIP)\n",
    "\n",
    "plt.plot(np.real(Z_exp), -np.imag(Z_exp), \"o\", markersize=10, color=\"black\", label=\"synth exp\")\n",
    "plt.plot(np.real(Z_DIP), -np.imag(Z_DIP), linewidth=4, color=\"red\", label=\"DP-DRT\")\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif', size=20)\n",
    "plt.annotate(r'$10^{-2}$', xy=(np.real(Z_exp[20]), -np.imag(Z_exp[20])), \n",
    "             xytext=(np.real(Z_exp[20])-2, 10-np.imag(Z_exp[20])), \n",
    "             arrowprops=dict(arrowstyle=\"-\",connectionstyle=\"arc\"))\n",
    "plt.annotate(r'$10^{-1}$', xy=(np.real(Z_exp[30]), -np.imag(Z_exp[30])), \n",
    "             xytext=(np.real(Z_exp[30])-2, 6-np.imag(Z_exp[30])), \n",
    "             arrowprops=dict(arrowstyle=\"-\",connectionstyle=\"arc\"))\n",
    "plt.annotate(r'$1$', xy=(np.real(Z_exp[40]), -np.imag(Z_exp[40])), \n",
    "             xytext=(np.real(Z_exp[40]), 10-np.imag(Z_exp[40])), \n",
    "             arrowprops=dict(arrowstyle=\"-\",connectionstyle=\"arc\"))\n",
    "plt.annotate(r'$10$', xy=(np.real(Z_exp[50]), -np.imag(Z_exp[50])), \n",
    "             xytext=(np.real(Z_exp[50])-1, 10-np.imag(Z_exp[50])), \n",
    "             arrowprops=dict(arrowstyle=\"-\",connectionstyle=\"arc\"))\n",
    "plt.rc('xtick', labelsize=15)\n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.legend(frameon=False, fontsize = 15)\n",
    "plt.xlim(10, 65)\n",
    "plt.ylim(0, 55)\n",
    "plt.xticks(range(0, 70, 10))\n",
    "plt.yticks(range(0, 60, 10))\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlabel(r'$Z_{\\rm re}/\\Omega$', fontsize = 20)\n",
    "plt.ylabel(r'$-Z_{\\rm im}/\\Omega$', fontsize = 20)\n",
    "plt.savefig('figs/Fig_2_a.eps', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figs/Fig_2_a.svg', dpi=300, bbox_inches='tight') \n",
    "fig = plt.gcf()\n",
    "size = fig.get_size_inches()\n",
    "plt.show()\n",
    "\n",
    "plt.semilogx(tau_vec, gamma_exact, linewidth=4, color=\"black\", label=\"exact\")\n",
    "plt.semilogx(tau_vec, gamma_DIP, linewidth=4, color=\"red\", label=\"early stop\")\n",
    "plt.semilogx(tau_vec, gamma_DIP_opt, linestyle='None', marker='o', color=\"blue\", label=\"optimal\")\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif', size=15)\n",
    "plt.rc('xtick', labelsize=15)\n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.axis([1E-4,1E4,-0.4,25])\n",
    "plt.legend(frameon=False, fontsize = 15)\n",
    "plt.xlabel(r'$\\tau/{\\rm s}$', fontsize = 20)\n",
    "plt.ylabel(r'$\\gamma/\\Omega$', fontsize = 20)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(5, 4)\n",
    "plt.savefig('figs/Fig_2_b.eps', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figs/Fig_2_b.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(loss_vec, linewidth=4, color=\"black\")\n",
    "plt.semilogy(np.array([index_early_stop[0], index_early_stop[0]]), np.array([1E-3, 1E7]), \n",
    "              ':', linewidth=3, color=\"red\")\n",
    "plt.semilogy(np.array([index_opt, index_opt]), np.array([1E-3, 1E7]), \n",
    "              ':', linewidth=3, color=\"blue\")\n",
    "plt.text(30000, 1E2, r'early stop', \n",
    "         {'color': 'red', 'fontsize': 20, 'ha': 'center', 'va': 'center', \n",
    "          'rotation': 90, \n",
    "          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"red\", pad=0.2)})\n",
    "plt.text(0.93E5, 1E2, r'optimal', \n",
    "         {'color': 'blue', 'fontsize': 20, 'ha': 'center', 'va': 'center', \n",
    "          'rotation': 90, \n",
    "          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"blue\", pad=0.2)})\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif', size=15)\n",
    "plt.rc('xtick', labelsize=15)\n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.xlabel(r'iter', fontsize=20)\n",
    "plt.ylabel(r'loss', fontsize=20)\n",
    "plt.axis([0,1.01E5,0.9E-2,1.1E6])\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(5, 4)\n",
    "plt.savefig('figs/Fig_2_c.eps', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figs/Fig_2_c.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(distance_vec, linewidth=4, color=\"black\")\n",
    "plt.semilogy(np.array([index_early_stop[0], index_early_stop[0]]), np.array([1E-3, 1E7]), \n",
    "              ':', linewidth=4, color=\"red\")\n",
    "plt.semilogy(np.array([index_opt, index_opt]), np.array([1E-3, 1E7]), \n",
    "              ':', linewidth=4, color=\"blue\")\n",
    "plt.text(30000, 2E1, r'early stop', \n",
    "         {'color': 'red', 'fontsize': 20, 'ha': 'center', 'va': 'center', \n",
    "          'rotation': 90, \n",
    "          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"red\", pad=0.2)})\n",
    "plt.text(0.93E5, 2E1, r'optimal', \n",
    "         {'color': 'blue', 'fontsize': 20, 'ha': 'center', 'va': 'center', \n",
    "          'rotation': 90, \n",
    "          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"blue\", pad=0.2)})\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif', size=15)\n",
    "plt.rc('xtick', labelsize=15)\n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.xlabel(r'iter', fontsize=20)\n",
    "plt.ylabel(r'error', fontsize=20)\n",
    "plt.axis([0,1.01E5,0.9E0,1.1E2])\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(5, 4)\n",
    "plt.savefig('figs/Fig_2_d.eps', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figs/Fig_2_d.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('total number parameters = ', compute_DRT.count_parameters(model))\n",
    "print('distance_early_stop = ', distance_vec[index_early_stop[0]])\n",
    "print('distance_opt= ', distance_vec[index_opt])\n",
    "\n",
    "PATH = os.getcwd()\n",
    "torch.save(model.state_dict(), 'model_wo_lambda.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_pytorch",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
